'''
You've encountered quite a few open source projects in the previous video. There's Hadoop, Hive, and PySpark. It's easy to get confused between these projects.

They have a few things in common: they are all currently maintained by the Apache Software Foundation, and they've all been used for massive parallel processing. Can you spot the differences?

'''


# Hadoop : Collection of open source pkgs, MapReduce is part of it, HDFS
# PySpark : Interface for Spark, DF abstraction
# Hive : Initially used Hadoop Map Reduce, build from the need to use structQueries
